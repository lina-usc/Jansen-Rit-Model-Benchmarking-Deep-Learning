{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers,  regularizers\n",
    "import pickle\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import scipy.stats\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def reset_random_seeds(seed_value=1234):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    python_random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    # Configure TensorFlow settings if necessary (rarely needed)\n",
    "    # tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    # tf.config.threading.set_inter_op_parallelism_threads(1\n",
    "\n",
    "# Function to create the LSTM model\n",
    "\n",
    "def create_bi_lstm_model(input_shape, lstm_units=64, dropout_rate=0.1, dense_units=8):\n",
    "    model = Sequential()\n",
    "    # First layer, needs to return sequences for subsequent layers\n",
    "    model.add(Bidirectional(\n",
    "        LSTM(lstm_units, return_sequences=False, dropout=dropout_rate,\n",
    "             kernel_initializer=initializers.GlorotUniform(seed=4287), \n",
    "        ),\n",
    "             bias_initializer=initializers.Constant(0.001)),\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "\n",
    "    # Final output layer\n",
    "    model.add(Dense(dense_units, activation='linear', \n",
    "                    kernel_initializer=initializers.GlorotUniform(seed=4287),\n",
    "                    bias_initializer=initializers.Constant(0.001)))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to compile and train the model\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs,  batch_size ):\n",
    "    early_stop = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    min_delta=0.01,       # an improvement significant if it's greater than 0.001\n",
    "    patience=50,           # Number of epochs to wait after the last improvement\n",
    "    verbose=1,             # Print messages when stopping\n",
    "    mode='min',            # Stop training when the quantity monitored has stopped decreasing\n",
    "    restore_best_weights=True ) # Restore model weights from the epoch with the best value of the monitored quantity.)\n",
    "    model.compile(optimizer='adam', loss=snr_inv_loss_db, metrics=['mse'])\n",
    "    history = model.fit(X_train, y_train, epochs = epochs, \n",
    "                        batch_size = batch_size,callbacks=[early_stop], validation_data=(X_val, y_val))\n",
    "    return history\n",
    "\n",
    "def snr_inv_loss_db(y_true, y_pred):\n",
    "    # Calculate the signal power (mean squared value of the true signal)\n",
    "    signal_power = tf.reduce_mean(tf.square(y_true))\n",
    "\n",
    "    # Calculate the noise power (mean squared error)\n",
    "    noise_power = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    # Calculate the inverse SNR\n",
    "    snr_inv = noise_power / signal_power  # Inverse SNR\n",
    "    \n",
    "    # Convert inverse SNR to decibels\n",
    "    snr_inv_db = 10 * tf.math.log(snr_inv) / tf.math.log(10.0) \n",
    "\n",
    "    return snr_inv_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/Users/deepatilwani/Documents/Projects/DCM_withMNE/Apri13_2024/DeepLearning_Data/data_30Apr/all_parm_varied_noise/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Column names\n",
    "colmuns_to_call = ['all'] # predicting all parameters together\n",
    "\n",
    "col = ['A1', 'B1', 'a', 'b', 'a_1', 'a2', 'a3', 'a4']\n",
    "\n",
    "noise_fact = ['0_11','0_21','0_32','0_42', '0_53', '0_63', '0_74', '0_84', '0_95']\n",
    "\n",
    "pred_path = '/Users/deepatilwani/Documents/Projects/DCM_withMNE/Apri13_2024/DeepLearning_Data/data_30Apr/all_parm_varied_noise/predictions/'\n",
    "\n",
    "#save the results in the path\n",
    "results_file_path = f\"{pred_path}_correlation_results.txt\"\n",
    "#define for the functions\n",
    "epochs=150\n",
    "batch_size=32\n",
    "liftby = 10\n",
    " \n",
    "    # Loop over each parameter\n",
    "for parameter in colmuns_to_call :\n",
    "    for noise in noise_fact:\n",
    "        reset_random_seeds()  # Reset the seeds\n",
    "        \n",
    "        # Load the X data\n",
    "        X_file_path = f'{path}evoked_xarr_noise_{parameter}_1000_{noise}.nc'\n",
    "\n",
    "        X_data = xr.open_dataset(X_file_path)\n",
    "\n",
    "        \n",
    "        drop_y = np.where(np.isnan(X_data).sum([\"ch_names\", \"time\"]).to_array().squeeze())[0]\n",
    "\n",
    "\n",
    "        X = X_data.dropna('sim_no').to_array().squeeze().to_numpy()\n",
    "\n",
    "        # Load the y data\n",
    "        y_file_path = f'{path}y_noise_{parameter}_1000_{noise}.pickle'\n",
    "        \n",
    "        with open(y_file_path, 'rb') as handle:\n",
    "            y_dict = pickle.load(handle)\n",
    "            # Assuming the dictionary contains a single key-value pair and the value is what you want\n",
    "            y = pd.DataFrame(y_dict)\n",
    "\n",
    "        y = y.drop(drop_y) \n",
    "\n",
    "\n",
    "        X_transformed = X.transpose(0,2,1) #samples X times X channels\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # Scale the data: Fit and transform the scaler to only the specified parameter\n",
    "        y_scaled = scaler.fit_transform(y)  \n",
    "\n",
    "        # Create a DataFrame for the scaled data\n",
    "        y_scaled_df = pd.DataFrame(y_scaled)\n",
    "            \n",
    "        lift_scale = 10**liftby     # for avoiding gradient descent vanishing probelm \n",
    "\n",
    "\n",
    "        #testing and training split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_transformed*lift_scale, \n",
    "                                                            y, test_size=0.10, random_state=68)\n",
    "        \n",
    "        #for validation split\n",
    "        X_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=68)\n",
    "        \n",
    "        # Create the LSTM model\n",
    "        model =  create_bi_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        \n",
    "        # Train the model\n",
    "        history = train_model(model, X_train, y_train, X_test, y_test, epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss_mse_results = model.evaluate(x_val, y_val, batch_size=batch_size)\n",
    "        \n",
    "        print(f'Results for {parameter}: Loss = {loss_mse_results[0]}, MSE = {loss_mse_results[1]}')\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = model.predict(x_val)\n",
    "        predictions_original_scale = scaler.inverse_transform(predictions)\n",
    "\n",
    "        y_val_original = scaler.inverse_transform(y_val)\n",
    "\n",
    "        #save predictions for future debugging\n",
    "        df_pred = pd.DataFrame()\n",
    "        df_pred = pd.DataFrame(predictions_original_scale, columns=col)\n",
    "        df_orig = pd.DataFrame(y_val_original, columns=col)\n",
    "        # Convert predictions to a DataFrame\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df_pred.to_csv(f'{pred_path}predictions_{parameter}.csv', index=False)\n",
    "        df_orig .to_csv(f'{pred_path}orignal_{parameter}.csv', index=False)\n",
    "\n",
    "        # Assuming df_pred and df_orig have the same columns for which you want to compute correlations\n",
    "        column_correlations = {}\n",
    "\n",
    "        for column in df_pred.columns:  # Loop through each column name\n",
    "            # Calculate Spearman correlation for each column\n",
    "            correlation, p_value = scipy.stats.spearmanr(df_pred[column], df_orig[column])\n",
    "            \n",
    "            # Prepare and save the results\n",
    "            result_string = f\"Correlation for {column}: r = {correlation:.3f}, p-value = {p_value:.3f}\\n\"\n",
    "            column_correlations[column] = result_string\n",
    "\n",
    "        with open(results_file_path, 'a') as file:    \n",
    "\n",
    "            # You can now print or write these results to a file\n",
    "            for column, result in column_correlations.items():\n",
    "                \n",
    "                result_string = f\"-- For {noise}\"\n",
    "\n",
    "                file.write(result_string)\n",
    "                file.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
